"""

@author: Jinal Shah

This script is for the webpage
front-end for this project.

The front-end will be simple with the following 
feature:

A text box for a user to enter text to be fed into 
the model. 

"""
# Importing libraries
import re
import streamlit as st 
import pandas as pd
import torch
import torch.nn as nn
import torchtext
from torchtext.data import get_tokenizer
from nltk.stem import SnowballStemmer
from tensorflow import keras
from keras.utils import pad_sequences
import sys
sys.path.append('src/')

from transformer import Model, PositionalEncoding, TokenEmbedding

st.markdown("<h1 style='text-align: center; color: teal;'>Verify.AI</h1>", unsafe_allow_html=True)
st.markdown("<h2 style='text-align: center; color: teal;'>Using machine learning to detect A.I generated essays ðŸ¤–</h2>", unsafe_allow_html=True)
st.markdown("<h3 style='text-align: center; color: teal;'>How It Works</h3>", unsafe_allow_html=True)
st.write('Paste your essay into the below text box and click submit. Within 3 minutes, you will see a probability of how likely your essay is generated by A.I')

# Creating state for inputted essay
if 'essay' not in st.session_state.keys():
    st.session_state['essay'] = ''

# Creating function to return stemmer, tokenizer, and vocab
@st.cache_resource
def get_all():
    tokenizer = get_tokenizer('spacy','en_core_web_sm')
    stemmer = SnowballStemmer(language='english')
    vocab = torch.load('vocab.pt')
    return tokenizer, stemmer, vocab

# Adding a caching function for the model
@st.cache_resource
def load_model():
    return torch.load('models/Transformer/transformers_30_epochs.pt',map_location=torch.device('cpu'))

# Adding a caching function for the embedding
@st.cache_resource
def load_embedding():
    return torch.load('models/Transformer/embedding_30_epochs.pt',map_location=torch.device('cpu'))

# Adding a caching function for the positional encoding
@st.cache_resource
def load_encoder():
    return torch.load('models/Transformer/positional_encoding_30_epochs.pt',map_location=torch.device('cpu'))

# Getting the preprocessing stuff
tokenizer, stemmer, vocab = get_all()

# Setting up the model
model = Model(d_model=512,nheads=8,dim_feedforward=2048,dropout=0.1,num_layers=2)
model.load_state_dict(load_model())
model.eval()
positional_encoder = PositionalEncoding(512,dropout=0.1,maxlen=500)
positional_encoder.load_state_dict(load_encoder())
positional_encoder.eval()
embedding = TokenEmbedding(vocab.__len__(),512)
embedding.load_state_dict(load_embedding())
embedding.eval()

# Function for preprocessing essays
def preprocess_essays(essay:str) -> list:
    prepared_essay = []
    # Preprocessing the essay a bit
    preprocess_essay = essay.replace('\n',"")
    preprocess_essay = essay.replace('\t',"")
    tokenized = tokenizer(preprocess_essay.replace('\n',""))

    # Getting the lemmas
    prepared_essay = [stemmer.stem(token) for token in tokenized]

    return prepared_essay

# Function to create padding mask
def create_padding_mask(X):
    return (X == vocab['<pad>'])

# Creating a form to drop the text into 
form = st.form(key='my_form')

# Need to make sure the \n are recognized as escape characters
st.session_state['text'] = form.text_input(label='Enter the Essay:').replace(r'\n','\n')
submit_button = form.form_submit_button('Submit')

# If button clicked
if submit_button:
    # Sending text through the preprocessing pipeline
    tokenized_essays = preprocess_essays(st.session_state['text'])
    preprocessed_input = [vocab(tokenized_essays)]

    # Padding the essay
    padded_essays = pad_sequences(preprocessed_input,maxlen=500,padding='post',truncating='post',value=vocab['<pad>'])
    padded_essays_tensor = torch.from_numpy(padded_essays)

    # Putting example through model
    embed_input = embedding(padded_essays_tensor)
    model_input = positional_encoder(embed_input)
    predictions = model(model_input,src_key_padding_mask=create_padding_mask(padded_essays_tensor))

    # Making predictions
    st.write(f'There is a {round(predictions.item() * 100,2)}% chance this essay was written by a LLM')
    st.write("Note: this number is simply a prediction. It can be incorrect ,and one shouldn't use this detector as a primary means of determining essay authorship!")
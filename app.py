"""

@author: Jinal Shah

This script is for the webpage
front-end for this project.

The front-end will be simple with the following 
feature:

A text box for a user to enter text to be fed into 
the model. 

"""
# Importing libraries
import re
import streamlit as st 
import pandas as pd
import torch
import torch.nn as nn
import torchtext
from torchtext.data import get_tokenizer
from nltk.stem import SnowballStemmer
from tensorflow import keras
from keras.utils import pad_sequences
import sys
sys.path.append('src/')

from transformer import Model, PositionalEncoding, TokenEmbedding
import subprocess

@st.cache_resource
def download_en_core_web_sm():
    subprocess.run(["python", "-m", "spacy", "download", "en_core_web_sm"])

# Downloading tokenizer
download_en_core_web_sm()

st.markdown("<h1 style='text-align: center; color: teal;'>Verify.AI</h1>", unsafe_allow_html=True)
st.markdown("<h2 style='text-align: center; color: teal;'>Using machine learning to detect A.I generated essays ðŸ¤–</h2>", unsafe_allow_html=True)
st.markdown("<h3 style='text-align: center; color: teal;'>How It Works</h3>", unsafe_allow_html=True)
st.write('Paste your essay into the below text box and click submit. Within 3 minutes, you will see a probability of how likely your essay is generated by A.I')

# Creating state for inputted essay
if 'essay' not in st.session_state.keys():
    st.session_state['essay'] = ''

# Creating states for tokenizer, stemmer, and vocab
if 'tokenizer' not in st.session_state.keys():
    st.session_state['tokenizer'] = get_tokenizer('spacy','en_core_web_sm')

if 'stemmer' not in st.session_state.keys():
    st.session_state['stemmer'] = SnowballStemmer(language='english')

if 'vocab' not in st.session_state.keys():
    st.session_state['vocab'] = torch.load('vocab.pt')

# Setting up the model
if 'model' not in st.session_state.keys():
    st.session_state['model'] = Model(d_model=512,nheads=8,dim_feedforward=2048,dropout=0.1,num_layers=2)
    st.session_state['model'].load_state_dict(torch.load('models/Transformer/transformers_30_epochs.pt',map_location=torch.device('cpu')))
    st.session_state['model'].eval() # putting model on evaluation mode

if 'positional_encoder' not in st.session_state.keys():
    st.session_state['positional_encoder'] = PositionalEncoding(512,dropout=0.1,maxlen=500)
    st.session_state['positional_encoder'].load_state_dict(torch.load('models/Transformer/positional_encoding_30_epochs.pt',map_location=torch.device('cpu')))
    st.session_state['positional_encoder'].eval() # putting model on evaluation mode

if 'embedding' not in st.session_state.keys():
    st.session_state['embedding'] = TokenEmbedding(st.session_state['vocab'].__len__(),512)
    st.session_state['embedding'].load_state_dict(torch.load('models/Transformer/embedding_30_epochs.pt',map_location=torch.device('cpu')))
    st.session_state['embedding'].eval() # putting model on evaluation mode

# Function for preprocessing essays
def preprocess_essays(essay:str) -> list:
    prepared_essay = []
    # Preprocessing the essay a bit
    preprocess_essay = essay.replace('\n',"")
    preprocess_essay = essay.replace('\t',"")
    tokenized = st.session_state['tokenizer'](preprocess_essay.replace('\n',""))

    # Getting the lemmas
    prepared_essay = [st.session_state['stemmer'].stem(token) for token in tokenized]

    return prepared_essay

# Function to create padding mask
def create_padding_mask(X):
    return (X == st.session_state['vocab']['<pad>'])

# Creating a form to drop the text into 
form = st.form(key='my_form')

# Need to make sure the \n are recognized as escape characters
st.session_state['text'] = form.text_input(label='Enter the Essay:').replace(r'\n','\n')
submit_button = form.form_submit_button('Submit')

# If button clicked
if submit_button:
    # Sending text through the preprocessing pipeline
    tokenized_essays = preprocess_essays(st.session_state['text'])
    preprocessed_input = [st.session_state['vocab'](tokenized_essays)]

    # Padding the essay
    padded_essays = pad_sequences(preprocessed_input,maxlen=500,padding='post',truncating='post',value=st.session_state['vocab']['<pad>'])
    padded_essays_tensor = torch.from_numpy(padded_essays)

    # Putting example through model
    embed_input = st.session_state['embedding'](padded_essays_tensor)
    model_input = st.session_state['positional_encoder'](embed_input)
    predictions = st.session_state['model'](model_input,src_key_padding_mask=create_padding_mask(padded_essays_tensor))

    # Making predictions
    st.write(f'There is a {round(predictions.item() * 100,2)}% chance this essay was written by a LLM')
    st.write("Note: this number is simply a prediction. It can be incorrect ,and one shouldn't use this detector as a primary means of determining essay authorship!")
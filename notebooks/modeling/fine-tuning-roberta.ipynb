{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":2795202,"sourceType":"datasetVersion","datasetId":575905},{"sourceId":6865136,"sourceType":"datasetVersion","datasetId":3945154},{"sourceId":7393250,"sourceType":"datasetVersion","datasetId":4298086},{"sourceId":7396904,"sourceType":"datasetVersion","datasetId":4300801},{"sourceId":7396931,"sourceType":"datasetVersion","datasetId":4300822},{"sourceId":7417511,"sourceType":"datasetVersion","datasetId":4309863},{"sourceId":7452336,"sourceType":"datasetVersion","datasetId":4310497},{"sourceId":7460359,"sourceType":"datasetVersion","datasetId":4342153}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LLM Competition Submission\n\nThis notebook represents my submission to the [LLM - Detect AI Generated Text](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/overview) competition on Kaggle. This notebook was made on Kaggle as the competition is a code competition. This is why there may be some inconsistencies with the other code files and notebooks.","metadata":{}},{"cell_type":"code","source":"# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom datasets import Dataset, DatasetDict\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\ntqdm.pandas()\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Getting the data\n# training_data = pd.read_csv('../input/prepared-data-llm-competition/prepared_training_set.csv')\n# training_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Renaming the columns\n# training_data.rename(columns={'essay':'text','LLM_written':'labels'},inplace=True)\n\n# # Changing labels to float\n# training_data['labels'] = training_data['labels'].astype('float')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Putting training data into a Dataset for Hugging Face\n# training = Dataset.from_pandas(training_data[['text','labels']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Getting the tokenizer and model\n# tokenizer = AutoTokenizer.from_pretrained('../input/roberta-base')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Function for tokenizing\n# def tokenize_function(example):\n#     return tokenizer(example['text'],padding='max_length',truncation=True,max_length=512)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Tokenizing\n# tokenized_data = training.map(tokenize_function,batched=True,batch_size=128)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the model\nmodel = AutoModelForSequenceClassification.from_pretrained('../input/roberta-base',num_labels=1)\nmodel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training_args = TrainingArguments(output_dir='test_trainer',evaluation_strategy='no',learning_rate=2e-5,weight_decay=0,num_train_epochs=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer = Trainer(model=model,args=training_args,train_dataset=tokenized_data,compute_metrics=roc_auc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Fine tuning\n# trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Saving the model\n# model.save_pretrained(\"fine-tuned-roberta\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Zipping the model\n# !zip -r roberta.zip /kaggle/working/fine-tuned-roberta","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Defining a function for inference\n# def inference(essay:str) -> float:\n#   # Tokenizing the input essay\n#   inputs = tokenizer(essay,padding='max_length',truncation=True,max_length=512,return_tensors='pt').to(device)\n\n#   # Getting the logits\n#   with torch.no_grad():\n#     logits = model(**inputs).logits\n#     probability = nn.functional.sigmoid(logits)\n#   return probability","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Running the examples through the model\n# train_predictions = training_data['text'].progress_apply(inference)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Making predictions on training data and evaluating\n# print('ROC AUC on Training Set:')\n# roc_auc_score(training_data['labels'],train_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"# Getting the tokenizer and model\nmodel_path = '../input/fine-tuned-roberta-for-llm-detection/kaggle/working/fine-tuned-roberta'\ntokenizer = AutoTokenizer.from_pretrained('../input/roberta-base')\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\nmodel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a function for inference\ndef inference(essay:str) -> float:\n  # Tokenizing the input essay\n  inputs = tokenizer(essay,padding='max_length',truncation=True,max_length=512,return_tensors='pt').to(device)\n\n  # Getting the logits\n  with torch.no_grad():\n    logits = model(**inputs).logits\n    probability = nn.functional.sigmoid(logits)\n  return probability.item()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the submission set\ntesting_data = pd.read_csv('../input/llm-detect-ai-generated-text/test_essays.csv')\ntesting_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting predictions\ntest_predictions = testing_data['text'].progress_apply(inference)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combining predictions with ids\nsubmission = pd.DataFrame()\nsubmission['id'] = testing_data['id']\nsubmission['generated'] = test_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the DataFrame to a CSV file\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "This notebook will represent my experiments and what I learned. I tried doing this directly on Kaggle, but the notebook got messy very quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.preprocessing import RobustScaler \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import shap\n",
    "from sklearn.model_selection import cross_validate\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the training data\n",
    "data = pd.read_csv('../prepared_training_set.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some minor data processing\n",
    "\n",
    "I just need to perform some minor data processing such as scaling the numerical values and droping the essay and row_id columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns\n",
    "training_data = data.drop(['row_id','essay'],axis=1)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting numerical features for scaling\n",
    "numerical = ['word_count','stop_word_count','stop_word_ratio','unique_word_count','unique_word_ratio','count_question','count_exclamation',\n",
    "            'count_semi','count_colon','grammar_errors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using RobustScaler since I know that there are outliers and that the data distribution isn't normal\n",
    "# RobustScaler will use median and IQR instead of mean and standard deviation\n",
    "scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "training_data[numerical] = scaler.fit_transform(training_data[numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scalar_grammar.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into X & y\n",
    "train_X = training_data.drop(['LLM_written'],axis=1)\n",
    "train_y = training_data['LLM_written'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "The data is ready to be modeled!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining and training the model\n",
    "log_model = LogisticRegression(random_state=42,C=0.5)\n",
    "log_model.fit(train_X.values,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Making predictions on training data and evaluating\n",
    "print('ROC AUC on Training Set:')\n",
    "predictions = log_model.predict_proba(train_X.values)[:,1]\n",
    "roc_auc_score(train_y,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validating\n",
    "cross_val_scores = pd.DataFrame(cross_validate(LogisticRegression(random_state=42,C=0.5),\n",
    "                                train_X.values,train_y,scoring='roc_auc',cv=5))\n",
    "cross_val_scores['test_score'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "d_tree = DecisionTreeClassifier(criterion='gini',min_samples_leaf=20,random_state=42)\n",
    "d_tree.fit(train_X.values,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on training data and evaluating\n",
    "print('ROC AUC on Training Set:')\n",
    "predictions = d_tree.predict_proba(train_X.values)[:,1]\n",
    "roc_auc_score(train_y,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validating\n",
    "cross_val_scores = pd.DataFrame(cross_validate(\n",
    "    DecisionTreeClassifier(criterion='gini',min_samples_leaf=20,random_state=42),\n",
    "                                train_X.values,train_y,scoring='roc_auc',cv=5))\n",
    "cross_val_scores['test_score'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance and SHAP\n",
    "\n",
    "I found that the model scores high on the training set and cross validation but not as high on the test set (LB). In fact, the disparity is very large. After doing some research via the discussion posts for the competition, I found that the test data distribution must be very different than the training data distribution. The difference is what everyone is trying to figure out. \n",
    "\n",
    "[One post](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/discussion/452750) mentions that it might be some noise introduced into the test essays. The author ran an experiment using grammar/spelling errors as a predictor and found that the rule works really well for training essays, but not as well for test essays (LB). This indicates that there must be some noise in the grammar errors. The hosts must have added grammatical mistakes into the test essays for both classes. Thus, I want to check to see what features are dominating decisions for my models. I can then experiment with removing them to see if that better matches the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the mean SHAP values for the logistic regression model\n",
    "explainer = shap.LinearExplainer(log_model,train_X)\n",
    "shap_values = explainer(train_X)\n",
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above SHAP plot shows, grammar_errors has a very high SHAP. This means that ,on average, the model places heavy importance on the grammar_errors. If the test data has grammar noise, this would make sense as per why the models are performing poorly on the LB data and excellent on the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the mean SHAP values for the Decision Tree model\n",
    "explainer = shap.TreeExplainer(d_tree)\n",
    "shap_values = explainer(train_X)\n",
    "shap.plots.bar(shap_values[:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot also shows that grammar_errors play a big role in decision making of the decision tree. Both models show that they are dependent on grammar_errors highly. This is problematic if the test data has grammar noise. The noise is causing the models to perform poorly. Based on the post and my analysis, I do want to experiment with removing the grammar errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing grammar_errors column\n",
    "\n",
    "My hypothesis is that since the grammar_errors column contributes highly to the model decision-making, there must be some grammatical noise in the test essays. I will remove this column and evaluate my models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_gram_numerical = ['word_count','stop_word_count','stop_word_ratio','unique_word_count','unique_word_ratio','count_question','count_exclamation',\n",
    "            'count_semi','count_colon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new scalar for when I don't have grammar_errors\n",
    "training_data_no_gram = data.drop(['row_id','essay','grammar_errors'],axis=1)\n",
    "training_data_no_gram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "scale_no_gram = RobustScaler()\n",
    "training_data_no_gram[no_gram_numerical] = scale_no_gram.fit_transform(training_data[no_gram_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scalar_no_grammar.pkl', 'wb') as file:\n",
    "    pickle.dump(scale_no_gram, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into X & y\n",
    "train_X_no_gram = training_data_no_gram.drop(['LLM_written'],axis=1)\n",
    "train_y_no_gram = training_data_no_gram['LLM_written'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a new model\n",
    "log_reg_no_grammar = LogisticRegression(random_state=42,C=0.5)\n",
    "log_reg_no_grammar.fit(train_X_no_gram.values,train_y_no_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on training data and evaluating\n",
    "print('ROC AUC on Training Set:')\n",
    "predictions = log_reg_no_grammar.predict_proba(train_X_no_gram.values)[:,1]\n",
    "roc_auc_score(train_y_no_gram,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validating\n",
    "cross_val_scores = pd.DataFrame(cross_validate(LogisticRegression(random_state=42,C=0.5),\n",
    "                                train_X_no_gram.values,train_y_no_gram,scoring='roc_auc',cv=5))\n",
    "cross_val_scores['test_score'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a slight amount of overfitting still, but the idea that the model doesn't perform as high on the validation sets is promising. Perhaps I might see a performance boost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the model\n",
    "d_tree = DecisionTreeClassifier(criterion='gini',min_samples_leaf=20,random_state=42)\n",
    "d_tree.fit(train_X_no_gram.values,train_y_no_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on training data and evaluating\n",
    "print('ROC AUC on Training Set:')\n",
    "predictions = d_tree.predict_proba(train_X_no_gram.values)[:,1]\n",
    "roc_auc_score(train_y_no_gram,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validating\n",
    "cross_val_scores = pd.DataFrame(cross_validate(DecisionTreeClassifier(criterion='gini',min_samples_leaf=20,random_state=42),\n",
    "                                train_X_no_gram.values,train_y_no_gram,scoring='roc_auc',cv=5))\n",
    "cross_val_scores['test_score'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update: I made submissions for both models. Both models performed terribly on the test set (0.51 and 0.478 respectively). The scores are as good as running a random classifier. Perhaps removing the grammar_errors column isn't good. Let me revert back to the original feature set and try running random forest on it. Maybe a more powerful model will help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "random_forest.fit(train_X.values,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on training data and evaluating\n",
    "print('ROC AUC on Training Set:')\n",
    "predictions = random_forest.predict_proba(train_X.values)[:,1]\n",
    "roc_auc_score(train_y,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validating\n",
    "cross_val_scores = pd.DataFrame(cross_validate(RandomForestClassifier(random_state=42),\n",
    "                                train_X_no_gram.values,train_y_no_gram,scoring='roc_auc',cv=5))\n",
    "cross_val_scores['test_score'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update: Random Forest performed really well (0.769). Perhaps removing grammar_errors isn't a good idea. I think I should focus my efforts on utilizing more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost\n",
    "\n",
    "Random Forest was promising, let's try gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building catboost model\n",
    "catboost_clf = CatBoostClassifier(iterations=100,learning_rate=0.03,loss_function='Logloss',\n",
    "                                 random_seed=42)\n",
    "catboost_clf.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on training data and evaluating\n",
    "print('ROC AUC on Training Set:')\n",
    "predictions = catboost_clf.predict_proba(train_X.values)[:,1]\n",
    "roc_auc_score(train_y,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detector\n",
    "\n",
    "I want to see how well the detector works on the test dataset. This could give me insight on how to utilize it better or if I shouldn't be using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding columns from the OpenAI detector\n",
    "# Getting the GPU\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # Need to put on GPU\n",
    "\n",
    "# Getting model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base-openai-detector\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base-openai-detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for inference\n",
    "def detector_pred(essay:str) -> float:\n",
    "  # Tokenizing the input essay\n",
    "  inputs = tokenizer(essay,return_tensors='pt',truncation=True).to(device)\n",
    "\n",
    "  # Getting the logits\n",
    "  with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "    probabilities = torch.nn.functional.softmax(logits)[:,0]\n",
    "  # Doing 1 - max logit because the model has \"Real\" = class 1 and \"Fake\" = class 0\n",
    "  # My labels are the opposite, 1 = LLM Written and 0 = student written.\n",
    "  # If a logit = 0 = Fake, 1-0 = 1 = LLM Written\n",
    "  # If a logit = 1 = Real, 1-1 = 0 = student written\n",
    "  return probabilities.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the probability predictions\n",
    "detector_test = data.copy()\n",
    "detector_test['generated'] = data['essay'].progress_apply(detector_pred)\n",
    "detector_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on training data and evaluating\n",
    "print('ROC AUC on Training Set:')\n",
    "roc_auc_score(train_y,detector_test['generated'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update: Just using the detector turned my score from 0.769 to .789. This gives me some intuition that maybe a deep learning approach is needed. The classical ML approach seems to cap at 0.75 to 0.77."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "This section got a little messy. A big takeaway for me is to get the modeling done first, then worry about tuning the hyperparameters. With that being said, here are my learnings:\n",
    "\n",
    "1. The classical ML approach with engineered features worked with the training data. I got high training scores and high CV scores. However, when it was time to generalize to a dataset with a completely different distribution and some added noise, the models struggled mightly. It seems that they started to overfit a lot and capped at a test performance of 0.76-0.77. These models perform well, but I think I will need stronger models to rank up on the LB and have better performance. \n",
    "\n",
    "2. When I ran the detector, it out-performed every classical ML model on the test dataset. This shows that I should switch my focus to deep learning approaches as they seem more ideal. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
